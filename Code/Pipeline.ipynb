{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "JHmc6Zas8Z2t",
        "TBdlMNx6dxo1",
        "12ga2b3nAxbA",
        "xSR3qc-_KHba",
        "_ssM2A_ojGSz",
        "lf0R1e1Q-ikn",
        "6zk63HdAaRGD",
        "ylMTh7pO6m6d"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###mount"
      ],
      "metadata": {
        "id": "JHmc6Zas8Z2t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Va5JoEhiaq1x",
        "outputId": "788fa3ab-2438-4b08-873d-a4cc64029fc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###libraries\n"
      ],
      "metadata": {
        "id": "12ga2b3nAxbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install langchain-experimental\n",
        "!pip install langchainhub"
      ],
      "metadata": {
        "id": "v_fbUd0VAvW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI, SQLDatabase\n",
        "from langchain_experimental.sql import SQLDatabaseChain\n",
        "from langchain import OpenAI, ConversationChain"
      ],
      "metadata": {
        "id": "UkGnskhMA29a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from operator import itemgetter\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate,FewShotChatMessagePromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import SequentialChain, LLMChain\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain.pydantic_v1 import BaseModel, Field"
      ],
      "metadata": {
        "id": "rrTBbImJA4IZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LLM\n",
        "\n"
      ],
      "metadata": {
        "id": "xSR3qc-_KHba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = 'Your Token'\n",
        "llm = OpenAI(temperature=0)\n"
      ],
      "metadata": {
        "id": "JAzSM9kvA62p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "db = SQLDatabase.from_uri(\"sqlite:///Benchmark/Elastic_benchmark_issues_unstructured.db\")\n",
        "db_cfg = SQLDatabase.from_uri(\"sqlite:///Benchmark/Elastic_benchmark_issues_structured.db\")\n",
        "\n",
        "def retriever(query):\n",
        "  if(cfg):\n",
        "    print('cfg')\n",
        "    db_chain_cfg = SQLDatabaseChain.from_llm(llm, db_cfg, verbose=True,use_query_checker=True, top_k=40)#,memory=memory)\n",
        "    return db_chain_cfg.run(query)\n",
        "  else:\n",
        "      db_chain_normal = SQLDatabaseChain.from_llm(llm, db, verbose=True,use_query_checker=True, top_k=40)#,memory=memory)\n",
        "      return db_chain_normal.run(query)"
      ],
      "metadata": {
        "id": "mgLYElc_hPjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Query"
      ],
      "metadata": {
        "id": "_ssM2A_ojGSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"input\": \"Find unresolved issues with no activity in the last 6 months\",\n",
        "        \"output\": \"Select issue numbers of open issues with last activity date older than 6 months ago.\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Suggest existing labels to tag issue 18608?\",\n",
        "        \"output\": \"List all existing labels and find suitable one for issue 18608 based on its content\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Is issue 18102 and 18669 similar?\",\n",
        "        \"output\": \"Compare the exceptions, stack traces, and descriptions of issues 18102 and 18669 to determine similarity.\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Are there any issues similar to issue 18669?\",\n",
        "        \"output\": \"Identify issues with exceptions similar to those in issue 18669.\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"How many times did the internal grinder tests fail in issue 17852?\",\n",
        "        \"output\": \"Extract the number of internal grinder test failures mentioned in issue 17852.\"\n",
        "    }\n",
        "]\n"
      ],
      "metadata": {
        "id": "Tdfb9WVX2Tp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We now transform these to example messages\n",
        "example_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"ai\", \"{output}\"),\n",
        "    ]\n",
        ")\n",
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=examples,\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You need to extract query information from a database of issues.\n",
        "     Your task is to rephrase the question to make it more concise and direct, without altering its core intent or specificity.\n",
        "     Questions are of four intents: Yes/no, fact, summarization and list.\n",
        "     1. Yes/No: Change 'is/are/have there issues' to  check if there any issues with provided condition\n",
        "     2. List: Change 'Find issues' to \"List issue numbers\" with provided condition\n",
        "     3. Summarization: Summarize the contents from issue title, exceptions, body and labels\n",
        "     4. Fact: Extract the fact\n",
        "     Retain keywords and intent as it is.\n",
        "\n",
        "     Follow these examples:\"\"\"),\n",
        "    # Few shot examples\n",
        "    few_shot_prompt,\n",
        "    # New question\n",
        "    (\"user\", \"{question}\"),\n",
        "])\n",
        "question_gen = prompt | ChatOpenAI(temperature=0) | StrOutputParser()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNwhroHrjFAB",
        "outputId": "a5e172fb-76ce-4554-b8db-e7b6dd86d740"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Process Questions"
      ],
      "metadata": {
        "id": "CHQKKXExaFzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def process_questions(questions):\n",
        "    answers = []\n",
        "\n",
        "    for i, q in enumerate(questions):\n",
        "        #try:\n",
        "            if(change_query):\n",
        "              print('query')\n",
        "              q=question_gen.invoke({\"question\": q})\n",
        "\n",
        "\n",
        "            if(MT):\n",
        "              if(COVE):\n",
        "                print('mt,cove')\n",
        "                init_response=Cove(q)\n",
        "              else:\n",
        "                init_response=retriever(q)\n",
        "              answer=MT_answer(q,init_response)\n",
        "\n",
        "            elif(COVE):\n",
        "              print('Cove')\n",
        "              answer=Cove(q)\n",
        "            else:\n",
        "              answer=retriever(q)\n",
        "\n",
        "            answers.append(answer)\n",
        "            print(f'-----------Final-------------: {answer}\\n')\n",
        "\n",
        "        #except Exception as e:\n",
        "                #print(f\"An error occurred while processing question {i+1}: {e}\")\n",
        "                #answers.append(None)  # Append None if an error occurs or question is skipped\n",
        "\n",
        "    return answers"
      ],
      "metadata": {
        "id": "0sKqB3ZFaIP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def write_to_csv(file_path, questions, answers):\n",
        "    with open(file_path, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['Question', 'Answer'])\n",
        "        for question, answer in zip(questions, answers):\n",
        "            writer.writerow([question, answer])\n"
      ],
      "metadata": {
        "id": "a_Lc2SasaQqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Configure"
      ],
      "metadata": {
        "id": "VlDPHmNrettx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name=''\n",
        "\n",
        "cfg=True\n",
        "change_query=True\n",
        "COVE=True\n",
        "MT=True\n",
        "\n",
        "#process_questions(['Identify one team with highest open issues over 8 months'])\n",
        "\n",
        "if(cfg):\n",
        "  name+='_CFG'\n",
        "if(change_query):\n",
        "  name+='_Query'\n",
        "if(COVE):\n",
        "  name+='_COVE'\n",
        "if(MT):\n",
        "  name+='_MT'\n",
        "\n",
        "if(cfg and change_query and COVE and MT):\n",
        "  name='_CIMBUR'  ## or CHIME. CIMBUR is the old name of CHIME\n",
        "\n",
        "if not (cfg or change_query or COVE or MT):\n",
        "    name = '_LLM'\n",
        "\n",
        "print(name)\n",
        "\n",
        "\n",
        "#result_df=pd.read_csv('Result/Correctness.csv')\n",
        "#count_df=pd.read_csv('Correct_Count.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whzZzwSXeaDB",
        "outputId": "232e19c0-e657-41d6-ac0e-ce79ec3267d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_CIMBUR\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cove"
      ],
      "metadata": {
        "id": "6zk63HdAaRGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0) #, model_name=\"gpt-4\"\n",
        "\n",
        "input_variables = [\"query\"]\n",
        "base_response_output_key = \"base_response\"\n",
        "base_response_template = \"\"\"Question: {query} Answer:\"\"\"\n",
        "base_repsonse_prompt_template = PromptTemplate(\n",
        "    input_variables=input_variables, template=base_response_template\n",
        ")\n",
        "base_response_chain = LLMChain(\n",
        "    llm=llm, prompt=base_repsonse_prompt_template,output_key=base_response_output_key)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VqkjUL7Xklvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def verify(query):\n",
        "\n",
        "  input_variables = [\"query\"]\n",
        "  base_response_output_key = \"base_response\"\n",
        "  base_response_template = \"\"\"Question: {query} Answer:\"\"\"\n",
        "  base_repsonse_prompt_template = PromptTemplate(\n",
        "      input_variables=input_variables, template=base_response_template\n",
        "  )\n",
        "  base_response_chain = LLMChain(\n",
        "      llm=llm, prompt=base_repsonse_prompt_template,output_key=base_response_output_key)\n",
        "\n",
        "\n",
        "  if(cfg):\n",
        "    db = SQLDatabase.from_uri(\"sqlite:///Benchmark/Elastic_benchmark_issues_structured.db\")\n",
        "  else:\n",
        "     db = SQLDatabase.from_uri(\"sqlite:///Benchmark/Elastic_benchmark_issues_unstructured.db\")\n",
        "\n",
        "  db_chain_normal = SQLDatabaseChain.from_llm(llm, db, verbose=True,use_query_checker=True,top_k=40,output_key=base_response_output_key)\n",
        "  def normal_retriever(query):\n",
        "      return db_chain_normal.run(query)\n",
        "\n",
        "\n",
        "  plan_verifications_template = \"\"\"\n",
        "  Given the below Question and answer, generate a series of verification questions that test the factual claims in the original baseline response.\n",
        "  For example if part of a longform model response contains the statement “The 2 exception types found in the issue report are java.io.EOFException, AssertionError”, then one possible\n",
        "  verification question to check those data could be “can java.io.EOFException be found in existing issue reports. If there is issue number\n",
        "  and asking about this issue only, focus on it.”\n",
        "\n",
        "  Question: {query}\n",
        "  Answer: {base_response}\n",
        "\n",
        "  <fact in query and passage>, <verification question, generated by combining the query and the fact>\n",
        "\n",
        "  {format_instructions}\n",
        "  \"\"\"\n",
        "\n",
        "  class PlanVerificationsOutput(BaseModel):\n",
        "      query: str = Field(description=\"The user's query\", default=\"\")\n",
        "      base_response: str = Field(description=\"The response to the user's query\", default=\"\")\n",
        "      facts_and_verification_questions: dict[str, str] = Field(\n",
        "          description=\"Facts (as the dictionary keys) extracted from the response and verification questions related to the query (as the dictionary values)\"\n",
        "      , default=\"\")\n",
        "\n",
        "  try:\n",
        "    plan_verifications_output_parser = PydanticOutputParser(\n",
        "        pydantic_object=PlanVerificationsOutput\n",
        "    )\n",
        "    plan_verifications_prompt_template = PromptTemplate(\n",
        "        input_variables=input_variables + [base_response_output_key],\n",
        "        template=plan_verifications_template,\n",
        "        partial_variables={\n",
        "            \"format_instructions\": plan_verifications_output_parser.get_format_instructions()\n",
        "        },\n",
        "    )\n",
        "    plan_verifications_chain = LLMChain(\n",
        "        llm=llm,\n",
        "        prompt=plan_verifications_prompt_template,\n",
        "        output_key=\"output\",\n",
        "        output_parser=plan_verifications_output_parser,\n",
        "    )\n",
        "\n",
        "\n",
        "    answer_and_plan_verification = SequentialChain(\n",
        "        chains=[db_chain_normal, plan_verifications_chain],\n",
        "        input_variables=[\"query\"],\n",
        "        output_variables=[\"output\"],\n",
        "        verbose=True)\n",
        "\n",
        "    intermediate_result = answer_and_plan_verification.run(query)\n",
        "\n",
        "    claimed_facts = list(intermediate_result.facts_and_verification_questions.keys())\n",
        "    verification_questions = list(\n",
        "        intermediate_result.facts_and_verification_questions.values()\n",
        "    )\n",
        "    verify_results_str = \"\"\n",
        "    verify_input_variables = [\"question\"]\n",
        "    verify_output_key = \"answer\"\n",
        "    verify_template = \"\"\"{question}\"\"\"\n",
        "\n",
        "    verify_prompt_template = PromptTemplate(\n",
        "        input_variables=verify_input_variables, template=verify_template\n",
        "    )\n",
        "\n",
        "    #verify_chain = LLMChain(\n",
        "    #    llm=llm, prompt=verify_prompt_template, output_key=verify_output_key)\n",
        "\n",
        "\n",
        "    verify_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True,use_query_checker=True,output_key=verify_output_key)#,memory=memory)\n",
        "\n",
        "\n",
        "    # Answering verification questions independently\n",
        "    for i in range(len(verification_questions)):\n",
        "        claimed_fact = claimed_facts[i]\n",
        "        question = verification_questions[i]\n",
        "        answer = verify_chain.run(question)\n",
        "        answer = answer.lstrip(\"\\n\")\n",
        "        verify_results_str += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
        "    return intermediate_result, verify_results_str\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    return None, ''\n",
        "\n",
        "#intermediate_result, verify_results_str= verify('what is the exception in issue 18151',False)\n"
      ],
      "metadata": {
        "id": "UcaSyG_9aQfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Cove(query):\n",
        "  try:\n",
        "    intermediate_result,verify_results_str=verify(query)\n",
        "    if(intermediate_result==None):\n",
        "      return ''\n",
        "    final_response_input_variables = [\"query\", \"base_response\", \"verify_results\"]\n",
        "    final_response_template = \"\"\"Given the ORIGINAL_QUESTION and the ORIGINAL_RESPONSE,\n",
        "    revise the ORIGINAL_RESPONSE (if applicable) such that it is consistent with information in VERIFIED_SOURCE as answer for ORIGINAL_QUESTION.\n",
        "    Only keep consistent information.\n",
        "\n",
        "    <ORIGINAL_QUESTION>\n",
        "    {query}\n",
        "\n",
        "    <ORIGINAL_RESPONSE>\n",
        "    {base_response}\n",
        "\n",
        "    <VERIFIED_SOURCE>\n",
        "    {verify_results}\n",
        "\n",
        "    Final response:\n",
        "    \"\"\"\n",
        "    final_response_prompt_template = PromptTemplate(\n",
        "        input_variables=final_response_input_variables,\n",
        "        template=final_response_template,\n",
        "    )\n",
        "\n",
        "    final_response_chain = LLMChain(llm=llm, prompt=final_response_prompt_template)\n",
        "\n",
        "\n",
        "    final_response = final_response_chain.run(\n",
        "        query=intermediate_result.query,\n",
        "        base_response=intermediate_result.base_response,\n",
        "        # verify_results=verify_results_str,\n",
        "        # Update 2023-10-22: use results from internet search\n",
        "        verify_results=verify_results_str,\n",
        "    )\n",
        "    print(final_response)\n",
        "    return final_response\n",
        "  except Exception as e:\n",
        "    return ''\n"
      ],
      "metadata": {
        "id": "PjVOQqJCGU_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MT"
      ],
      "metadata": {
        "id": "ylMTh7pO6m6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "def mutate_query(original_query, n_mutations=3):\n",
        "\n",
        "        # Prompt for the LLM to generate mutations\n",
        "        template = \"\"\"\n",
        "        Generate 3 different ways to ask the following question, keeping the semantic meaning the same:\\n\\n'{input}'\\n\\nMutations:\n",
        "        text: {input}\n",
        "        \"\"\"\n",
        "        # Initialize the ChatOpenAI model\n",
        "\n",
        "        llm = OpenAI(temperature=0)\n",
        "        prompt_template = PromptTemplate.from_template(template=template)\n",
        "        chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "        response=chain.predict(input=original_query)\n",
        "        print(response)\n",
        "\n",
        "        # Extract and return the mutated queries\n",
        "        # Assuming the response format aligns with your expectations\n",
        "\n",
        "         # Assuming the response is a string, split and format it appropriately\n",
        "        mutated_queries = response.split(\"\\n\")  # Adjust this based on the actual format of 'response'\n",
        "\n",
        "        # Filter out empty lines or irrelevant parts\n",
        "        mutated_queries = [query.strip() for query in mutated_queries if query.strip() and not query.strip().startswith(\"Mutations\")]\n",
        "\n",
        "        return mutated_queries"
      ],
      "metadata": {
        "id": "H1snseUQ6pmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def MT_answer(original_query,init_response):\n",
        "  try:\n",
        "    mutated_queries = mutate_query(original_query)\n",
        "    print(mutated_queries)\n",
        "\n",
        "    verify_results_str =\"\"\n",
        "    for q in mutated_queries:\n",
        "      a=retriever(q)\n",
        "      verify_results_str+= f\"Question: {q}\\nAnswer: {a}\\n\\n\"\n",
        "\n",
        "    query = original_query\n",
        "    base_response = init_response\n",
        "\n",
        "\n",
        "    print('BASE--------')\n",
        "    print(base_response)\n",
        "    print('verify-----')\n",
        "    print(verify_results_str)\n",
        "\n",
        "\n",
        "    # Final response prompt template\n",
        "    final_response_input_variables = [\"query\", \"base_response\", \"verify_results\"]\n",
        "    final_response_template = \"\"\"Given the ORIGINAL_QUESTION and the ORIGINAL_RESPONSE,\n",
        "    revise the ORIGINAL_RESPONSE (if applicable) such that it is consistent with information in VERIFIED_SOURCE as answer for ORIGINAL_QUESTION.\n",
        "    Only keep consistent information.\n",
        "\n",
        "    <ORIGINAL_QUESTION>\n",
        "    {query}\n",
        "\n",
        "    <ORIGINAL_RESPONSE>\n",
        "    {base_response}\n",
        "\n",
        "    <VERIFIED_SOURCE>\n",
        "    {verify_results}\n",
        "\n",
        "    Final response:\n",
        "    \"\"\"\n",
        "\n",
        "    final_response_prompt_template = PromptTemplate(\n",
        "        input_variables=final_response_input_variables,\n",
        "        template=final_response_template,\n",
        "    )\n",
        "\n",
        "    final_response_chain = LLMChain(llm=llm, prompt=final_response_prompt_template)\n",
        "\n",
        "    # Running the chain to get the final response\n",
        "    final_response = final_response_chain.run(\n",
        "        query=query,\n",
        "        base_response=base_response,\n",
        "        verify_results=verify_results_str,\n",
        "    )\n",
        "\n",
        "    print('final-----')\n",
        "    print(\"Final Response:\", final_response)\n",
        "    return final_response\n",
        "  except Exception as e:\n",
        "    return ''\n"
      ],
      "metadata": {
        "id": "a8IYDJYg-Rlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Backlog"
      ],
      "metadata": {
        "id": "gqj73gCR2-oO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###y/n"
      ],
      "metadata": {
        "id": "lIE2mt__Ma8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Benchmark/E-T5.csv')\n",
        "\n",
        "single_q = df.loc[df['T5-YNQ'].notnull(), 'T5-YNQ']\n",
        "single_a=process_questions(single_q)\n",
        "\n",
        "df[f'T5-YNA{name}'] = ''\n",
        "df.loc[:len(single_a)-1, f'T5-YNA{name}'] = single_a\n",
        "\n",
        "df.to_csv('Benchmark/E-T5.csv', index=False)\n",
        "\n",
        "# Get actual and expected answers\n",
        "actual_answers = df.loc[df[f'T5-YNA{name}'].notnull(), f'T5-YNA{name}']\n",
        "expected_answers = df.loc[df['T5-YNA'].notnull(), 'T5-YNA']\n",
        "\n",
        "# Call the function\n",
        "results, correct, correctness = classify_and_evaluate(actual_answers, expected_answers)\n",
        "print(\"Correctness:\", correctness)\n",
        "\n",
        "df[f'T5-YN{name}_R'] = ''\n",
        "df[f'T5-YN{name}_R'][:len(results)] = results\n",
        "\n",
        "df.to_csv('Benchmark/E-T5.csv', index=False)\n",
        "\n",
        "result_df[f'T5-YN{name}']=[correctness]\n",
        "result_df.to_csv('Result/Correctness.csv', index=False)\n",
        "\n",
        "count_df[f'T5-YN{name}']=[correct]\n",
        "count_df.to_csv('Correct_Count.csv', index=False)"
      ],
      "metadata": {
        "id": "J7ySzbhZKWmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###sum"
      ],
      "metadata": {
        "id": "j6_j4wobMgVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Benchmark/E-T5.csv')\n",
        "\n",
        "single_q = df.loc[df['T5-SQ'].notnull(), 'T5-SQ']\n",
        "single_a=process_questions(single_q)\n",
        "\n",
        "df[f'T5-SA{name}'] = ''\n",
        "df.loc[:len(single_a)-1, f'T5-SA{name}'] = single_a\n",
        "\n",
        "df.to_csv('Benchmark/E-T5.csv', index=False)\n",
        "\n",
        "# Get actual and expected answers\n",
        "actual_answers = single_a\n",
        "expected_answers = df.loc[df['T5-SA'].notnull(), 'T5-SA']\n",
        "\n",
        "# Call the function\n",
        "results, correct, correctness = evaluate_summary(actual_answers, expected_answers)\n",
        "print(\"Correctness:\", correctness)\n",
        "\n",
        "df[f'T5-S{name}_R'] = ''\n",
        "df[f'T5-S{name}_R'][:len(results)] = results\n",
        "\n",
        "df.to_csv('Benchmark/E-T5.csv', index=False)\n",
        "\n",
        "result_df[f'T5-S{name}']=[correctness]\n",
        "result_df.to_csv('Result/Correctness.csv', index=False)\n",
        "\n",
        "count_df[f'T5-S{name}']=[correct]\n",
        "count_df.to_csv('Correct_Count.csv', index=False)"
      ],
      "metadata": {
        "id": "kmYOj7FTMmKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###fact"
      ],
      "metadata": {
        "id": "YAyeF9x7Mh3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv('Benchmark/E-T5.csv')\n",
        "\n",
        "single_q = df.loc[df['T5-FQ'].notnull(), 'T5-FQ']\n",
        "single_a=process_questions(single_q)\n",
        "\n",
        "df[f'T5-FA{name}'] = ''\n",
        "df.loc[:len(single_a)-1, f'T5-FA{name}'] = single_a\n",
        "\n",
        "df.to_csv('Benchmark/E-T5.csv', index=False)\n",
        "\n",
        "# Get actual and expected answers\n",
        "actual_answers = single_a\n",
        "expected_answers = df.loc[df['T5-FA'].notnull(), 'T5-FA']\n",
        "\n",
        "# Call the function\n",
        "results, correct, correctness = evaluate_summary(actual_answers, expected_answers)\n",
        "print(\"Correctness:\", correctness)\n"
      ],
      "metadata": {
        "id": "552b6s0ONGgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Label"
      ],
      "metadata": {
        "id": "wvb4KMPBgs5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###y/n"
      ],
      "metadata": {
        "id": "ijXz9RCkgwK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Benchmark/E-T4.csv')\n",
        "\n",
        "single_q = df.loc[df['T4-YNQ'].notnull(), 'T4-YNQ']\n",
        "single_a=process_questions(single_q)\n",
        "\n",
        "df[f'T4-YNA{name}'] = ''\n",
        "df.loc[:len(single_a)-1, f'T4-YNA{name}'] = single_a\n",
        "\n",
        "df.to_csv('Benchmark/E-T4.csv', index=False)\n",
        "\n",
        "# Get actual and expected answers\n",
        "actual_answers = df.loc[df[f'T4-YNA{name}'].notnull(), f'T4-YNA{name}']\n",
        "expected_answers = df.loc[df['T4-YNA'].notnull(), 'T4-YNA']\n",
        "\n",
        "# Call the function\n",
        "results, correct, correctness = classify_and_evaluate(actual_answers, expected_answers)\n",
        "print(\"Correctness:\", correctness)\n",
        "\n",
        "df[f'T4-YN{name}_R'] = ''\n",
        "df[f'T4-YN{name}_R'][:len(results)] = results\n",
        "\n",
        "\n",
        "df.to_csv('Benchmark/E-T4.csv', index=False)\n",
        "\n",
        "result_df[f'T4-YN{name}']=[correctness]\n",
        "result_df.to_csv('Result/Correctness.csv', index=False)\n",
        "\n",
        "count_df[f'T4-YN{name}']=[correct]\n",
        "count_df.to_csv('Correct_Count.csv', index=False)\n"
      ],
      "metadata": {
        "id": "qeHMnVd9gwK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###sum"
      ],
      "metadata": {
        "id": "quQnmO_ciH_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Benchmark/E-T4.csv')\n",
        "\n",
        "single_q = df.loc[df['T4-SQ'].notnull(), 'T4-SQ']\n",
        "single_a=process_questions(single_q)\n",
        "\n",
        "df[f'T4-SA{name}'] = ''\n",
        "df.loc[:len(single_a)-1, f'T4-SA{name}'] = single_a\n",
        "\n",
        "df.to_csv('Benchmark/E-T4.csv', index=False)\n",
        "\n",
        "# Get actual and expected answers\n",
        "actual_answers = single_a\n",
        "expected_answers = df.loc[df['T4-SA'].notnull(), 'T4-SA']\n",
        "\n",
        "# Call the function\n",
        "results, correct, correctness = evaluate_summary(actual_answers, expected_answers)\n",
        "print(\"Correctness:\", correctness)\n",
        "\n",
        "df[f'T4-S{name}_R'] = ''\n",
        "df[f'T4-S{name}_R'][:len(results)] = results\n",
        "\n",
        "df.to_csv('Benchmark/E-T4.csv', index=False)\n",
        "\n",
        "result_df[f'T4-S{name}']=[correctness]\n",
        "result_df.to_csv('Result/Correctness.csv', index=False)\n",
        "\n",
        "count_df[f'T4-S{name}']=[correct]\n",
        "count_df.to_csv('Correct_Count.csv', index=False)"
      ],
      "metadata": {
        "id": "HhwfGN2-iH_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###fact"
      ],
      "metadata": {
        "id": "skwRQd6Vh9Gz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv('Benchmark/E-T4.csv')\n",
        "\n",
        "single_q = df.loc[df['T4-FQ'].notnull(), 'T4-FQ']\n",
        "single_a=process_questions(single_q)\n",
        "\n",
        "df[f'T4-FA{name}'] = ''\n",
        "df.loc[:len(single_a)-1, f'T4-FA{name}'] = single_a\n",
        "\n",
        "df.to_csv('Benchmark/E-T4.csv', index=False)\n",
        "\n",
        "# Get actual and expected answers\n",
        "actual_answers = single_a\n",
        "expected_answers = df.loc[df['T4-FA'].notnull(), 'T4-FA']\n",
        "\n",
        "# Call the function\n",
        "results, correct, correctness = evaluate_summary(actual_answers, expected_answers)\n",
        "print(\"Correctness:\", correctness)\n",
        "\n",
        "df[f'T4-FA{name}_R'] = ''\n",
        "df[f'T4-FA{name}_R'][:len(results)] = results\n",
        "\n",
        "df.to_csv('Benchmark/E-T4.csv', index=False)\n",
        "\n",
        "result_df[f'T4-F{name}']=[correctness]\n",
        "result_df.to_csv('Result/Correctness.csv', index=False)\n",
        "\n",
        "count_df[f'T4-F{name}']=[correct]\n",
        "count_df.to_csv('Correct_Count.csv', index=False)"
      ],
      "metadata": {
        "id": "G2BrDw-lh9G0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Summary\n"
      ],
      "metadata": {
        "id": "zFgon7CZ5FPm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###sum"
      ],
      "metadata": {
        "id": "qpW1n7AFyEOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Benchmark/E-T3.csv')\n",
        "\n",
        "single_q = df.loc[df['T3-SQ'].notnull(), 'T3-SQ']\n",
        "single_a=process_questions(single_q)\n",
        "\n",
        "df[f'T3-SA{name}'] = ''\n",
        "df.loc[:len(single_a)-1, f'T3-SA{name}'] = single_a\n",
        "\n",
        "df.to_csv('Benchmark/E-T3.csv', index=False)\n",
        "\n",
        "# Get actual and expected answers\n",
        "actual_answers = single_a\n",
        "expected_answers = df.loc[df['T3-SA'].notnull(), 'T3-SA']\n",
        "\n",
        "# Call the function\n",
        "results, correct, correctness = evaluate_summary(actual_answers, expected_answers)\n",
        "print(\"Correctness:\", correctness)\n",
        "\n",
        "df[f'T3-S{name}_R'] = ''\n",
        "df[f'T3-S{name}_R'][:len(results)] = results\n",
        "\n",
        "df.to_csv('Benchmark/E-T3.csv', index=False)\n",
        "\n",
        "result_df[f'T3-S{name}']=[correctness]\n",
        "result_df.to_csv('Result/Correctness.csv', index=False)\n",
        "\n",
        "count_df[f'T3-S{name}']=[correct]\n",
        "count_df.to_csv('Correct_Count.csv', index=False)"
      ],
      "metadata": {
        "id": "BkcvcckGyEOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###fact"
      ],
      "metadata": {
        "id": "wgW8-Y2hDGBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Benchmark/E-T3.csv')\n",
        "\n",
        "single_q = df.loc[df['T3-FQ'].notnull(), 'T3-FQ']\n",
        "single_a=process_questions(single_q)\n",
        "\n",
        "df[f'T3-FA{name}'] = ''\n",
        "df.loc[:len(single_a)-1, f'T3-FA{name}'] = single_a\n",
        "\n",
        "df.to_csv('Benchmark/E-T3.csv', index=False)\n",
        "\n",
        "# Get actual and expected answers\n",
        "actual_answers = single_a\n",
        "expected_answers = df.loc[df['T3-FA'].notnull(), 'T3-FA']\n",
        "\n",
        "# Call the function\n",
        "results, correct, correctness = evaluate_summary(actual_answers, expected_answers)\n",
        "print(\"Correctness:\", correctness)\n",
        "\n",
        "df[f'T3-FA{name}_R'] = ''\n",
        "df[f'T3-FA{name}_R'][:len(results)] = results\n",
        "\n",
        "df.to_csv('Benchmark/E-T3.csv', index=False)\n",
        "\n",
        "\n",
        "result_df[f'T3-F{name}']=[correctness]\n",
        "result_df.to_csv('Result/Correctness.csv', index=False)\n",
        "\n",
        "count_df[f'T3-F{name}']=[correct]\n",
        "count_df.to_csv('Correct_Count.csv', index=False)"
      ],
      "metadata": {
        "id": "T2qF4QrODGBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Trend"
      ],
      "metadata": {
        "id": "9kmRoC0iTNX_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###y/n"
      ],
      "metadata": {
        "id": "S6QvEsvWTUuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Benchmark/E-T2.csv')\n",
        "\n",
        "single_q = df.loc[df['T2-YNQ'].notnull(), 'T2-YNQ']\n",
        "single_a=process_questions(single_q)\n",
        "\n",
        "df[f'T2-YNA{name}'] = ''\n",
        "df.loc[:len(single_a)-1, f'T2-YNA{name}'] = single_a\n",
        "\n",
        "df.to_csv('Benchmark/E-T2.csv', index=False)\n",
        "\n",
        "# Get actual and expected answers\n",
        "actual_answers = df.loc[df[f'T2-YNA{name}'].notnull(), f'T2-YNA{name}']\n",
        "expected_answers = df.loc[df['T2-YNA'].notnull(), 'T2-YNA']\n",
        "\n",
        "# Call the function\n",
        "results, correct, correctness = classify_and_evaluate(actual_answers, expected_answers)\n",
        "print(\"Correctness:\", correctness)\n",
        "\n",
        "df[f'T2-YN{name}_R'] = ''\n",
        "df[f'T2-YN{name}_R'][:len(results)] = results\n",
        "\n",
        "\n",
        "df.to_csv('Benchmark/E-T2.csv', index=False)\n",
        "\n",
        "result_df[f'T2-YN{name}']=[correctness]\n",
        "result_df.to_csv('Result/Correctness.csv', index=False)\n",
        "\n",
        "count_df[f'T2-YN{name}']=[correct]\n",
        "count_df.to_csv('Correct_Count.csv', index=False)\n"
      ],
      "metadata": {
        "id": "IfLPcRtLTUuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###sum"
      ],
      "metadata": {
        "id": "YjqXdHXBTUuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Benchmark/E-T2.csv')\n",
        "\n",
        "single_q = df.loc[df['T2-SQ'].notnull(), 'T2-SQ']\n",
        "single_a=process_questions(single_q)\n",
        "\n",
        "df[f'T2-SA{name}'] = ''\n",
        "df.loc[:len(single_a)-1, f'T2-SA{name}'] = single_a\n",
        "\n",
        "df.to_csv('Benchmark/E-T2.csv', index=False)\n",
        "\n",
        "# Get actual and expected answers\n",
        "actual_answers = single_a\n",
        "expected_answers = df.loc[df['T2-SA'].notnull(), 'T2-SA']\n",
        "\n",
        "# Call the function\n",
        "results, correct, correctness = evaluate_summary(actual_answers, expected_answers)\n",
        "print(\"Correctness:\", correctness)\n",
        "\n",
        "df[f'T2-S{name}_R'] = ''\n",
        "df[f'T2-S{name}_R'][:len(results)] = results\n",
        "\n",
        "df.to_csv('Benchmark/E-T2.csv', index=False)\n",
        "\n",
        "\n",
        "result_df[f'T2-S{name}']=[correctness]\n",
        "result_df.to_csv('Result/Correctness.csv', index=False)\n",
        "\n",
        "count_df[f'T2-S{name}']=[correct]\n",
        "count_df.to_csv('Correct_Count.csv', index=False)"
      ],
      "metadata": {
        "id": "jZFh1IHITUuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###fact"
      ],
      "metadata": {
        "id": "_j3MjoVzTUuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Benchmark/E-T2.csv')\n",
        "\n",
        "single_q = df.loc[df['T2-FQ'].notnull(), 'T2-FQ']\n",
        "single_a=process_questions(single_q)\n",
        "\n",
        "df[f'T2-FA{name}'] = ''\n",
        "df.loc[:len(single_a)-1, f'T2-FA{name}'] = single_a\n",
        "\n",
        "df.to_csv('Benchmark/E-T2.csv', index=False)\n",
        "\n",
        "# Get actual and expected answers\n",
        "actual_answers = single_a\n",
        "expected_answers = df.loc[df['T2-FA'].notnull(), 'T2-FA']\n",
        "\n",
        "# Call the function\n",
        "results, correct, correctness = evaluate_summary(actual_answers, expected_answers)\n",
        "print(\"Correctness:\", correctness)\n",
        "\n",
        "df[f'T2-FA{name}_R'] = ''\n",
        "df[f'T2-FA{name}_R'][:len(results)] = results\n",
        "\n",
        "df.to_csv('Benchmark/E-T2.csv', index=False)\n",
        "\n",
        "\n",
        "result_df[f'T2-F{name}']=[correctness]\n",
        "result_df.to_csv('Result/Correctness.csv', index=False)\n",
        "\n",
        "count_df[f'T2-F{name}']=[correct]\n",
        "count_df.to_csv('Correct_Count.csv', index=False)"
      ],
      "metadata": {
        "id": "gT1K57PqTUuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Similar"
      ],
      "metadata": {
        "id": "abevbzP_kVrG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###y/n"
      ],
      "metadata": {
        "id": "N0imq_TCkVrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Benchmark/E-T1M.csv')\n",
        "\n",
        "single_q = df.loc[df['T1M-YNQ'].notnull(), 'T1M-YNQ']\n",
        "single_a=process_questions(single_q)\n",
        "\n",
        "df[f'T1M-YNA{name}'] = ''\n",
        "df.loc[:len(single_a)-1, f'T1M-YNA{name}'] = single_a\n",
        "\n",
        "df.to_csv('Benchmark/E-T1M.csv', index=False)\n",
        "\n",
        "# Get actual and expected answers\n",
        "actual_answers = df.loc[df[f'T1M-YNA{name}'].notnull(), f'T1M-YNA{name}']\n",
        "expected_answers = df.loc[df['T1M-YNA'].notnull(), 'T1M-YNA']\n",
        "\n",
        "# Call the function\n",
        "results, correct, correctness = classify_and_evaluate(actual_answers, expected_answers)\n",
        "print(\"Correctness:\", correctness)\n",
        "\n",
        "df[f'T1M-YN{name}_R'] = ''\n",
        "df[f'T1M-YN{name}_R'][:len(results)] = results\n",
        "\n",
        "\n",
        "df.to_csv('Benchmark/E-T1M.csv', index=False)\n",
        "\n",
        "result_df[f'T1M-YN{name}']=[correctness]\n",
        "result_df.to_csv('Result/Correctness.csv', index=False)\n",
        "\n",
        "count_df[f'T1M-YN{name}']=[correct]\n",
        "count_df.to_csv('Correct_Count.csv', index=False)\n"
      ],
      "metadata": {
        "id": "sHrnfYQokVrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###sum"
      ],
      "metadata": {
        "id": "BgNL04V5kVrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Benchmark/E-T1M.csv')\n",
        "\n",
        "single_q = df.loc[df['T1M-SQ'].notnull(), 'T1M-SQ']\n",
        "single_a=process_questions(single_q)\n",
        "\n",
        "df[f'T1M-SA{name}'] = ''\n",
        "df.loc[:len(single_a)-1, f'T1M-SA{name}'] = single_a\n",
        "\n",
        "df.to_csv('Benchmark/E-T1M.csv', index=False)\n",
        "\n",
        "# Get actual and expected answers\n",
        "actual_answers = single_a\n",
        "expected_answers = df.loc[df['T1M-SA'].notnull(), 'T1M-SA']\n",
        "\n",
        "# Call the function\n",
        "results, correct, correctness = evaluate_summary(actual_answers, expected_answers)\n",
        "print(\"Correctness:\", correctness)\n",
        "\n",
        "df[f'T1M-S{name}_R'] = ''\n",
        "df[f'T1M-S{name}_R'][:len(results)] = results\n",
        "\n",
        "df.to_csv('Benchmark/E-T1M.csv', index=False)\n",
        "\n",
        "result_df[f'T1M-S{name}']=[correctness]\n",
        "result_df.to_csv('Result/Correctness.csv', index=False)\n",
        "\n",
        "count_df[f'T1M-S{name}']=[correct]\n",
        "count_df.to_csv('Correct_Count.csv', index=False)"
      ],
      "metadata": {
        "id": "BoVLjdBzkVrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###fact"
      ],
      "metadata": {
        "id": "jLkcQjEPkVrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Benchmark/E-T1M.csv')\n",
        "\n",
        "single_q = df.loc[df['T1M-FQ'].notnull(), 'T1M-FQ']\n",
        "single_a=process_questions(single_q)\n",
        "\n",
        "df[f'T1M-FA{name}'] = ''\n",
        "df.loc[:len(single_a)-1, f'T1M-FA{name}'] = single_a\n",
        "\n",
        "df.to_csv('Benchmark/E-T1M.csv', index=False)\n",
        "\n",
        "# Get actual and expected answers\n",
        "actual_answers = single_a\n",
        "expected_answers = df.loc[df['T1M-FA'].notnull(), 'T1M-FA']\n",
        "\n",
        "# Call the function\n",
        "results, correct, correctness = evaluate_summary(actual_answers, expected_answers)\n",
        "print(\"Correctness:\", correctness)\n",
        "\n",
        "df[f'T1M-FA{name}_R'] = ''\n",
        "df[f'T1M-FA{name}_R'][:len(results)] = results\n",
        "\n",
        "df.to_csv('Benchmark/E-T1M.csv', index=False)\n",
        "\n",
        "\n",
        "\n",
        "result_df[f'T1M-F{name}']=[correctness]\n",
        "result_df.to_csv('Result/Correctness.csv', index=False)\n",
        "\n",
        "count_df[f'T1M-F{name}']=[correct]\n",
        "count_df.to_csv('Correct_Count.csv', index=False)"
      ],
      "metadata": {
        "id": "qcn0DgwikVrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Single"
      ],
      "metadata": {
        "id": "6G5fnIx_7BvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###y/n"
      ],
      "metadata": {
        "id": "x_7hu1-yxAX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Benchmark/E-T1S.csv')\n",
        "\n",
        "single_q = df.loc[df['T1S-YNQ'].notnull(), 'T1S-YNQ']\n",
        "single_a=process_questions(single_q)\n",
        "\n",
        "df[f'T1S-YNA{name}'] = ''\n",
        "df.loc[:len(single_a)-1, f'T1S-YNA{name}'] = single_a\n",
        "\n",
        "df.to_csv('Benchmark/E-T1S.csv', index=False)\n",
        "\n",
        "# Get actual and expected answers\n",
        "actual_answers = df.loc[df[f'T1S-YNA{name}'].notnull(), f'T1S-YNA{name}']\n",
        "expected_answers = df.loc[df['T1S-YNA'].notnull(), 'T1S-YNA']\n",
        "\n",
        "# Call the function\n",
        "results, correct, correctness = classify_and_evaluate(actual_answers, expected_answers)\n",
        "print(\"Correctness:\", correctness)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sZGxHqHt8Nlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[f'T1S-YN{name}_R'] = ''\n",
        "df[f'T1S-YN{name}_R'][:len(results)] = results\n",
        "\n",
        "\n",
        "df.to_csv('Benchmark/E-T1S.csv', index=False)\n",
        "\n",
        "result_df[f'T1S-YN{name}']=[correctness]\n",
        "result_df.to_csv('Result/Correctness.csv', index=False)\n",
        "\n",
        "count_df[f'T1S-YN{name}']=[correct]\n",
        "count_df.to_csv('Correct_Count.csv', index=False)"
      ],
      "metadata": {
        "id": "Zn9dfQ0yV7qI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###sum"
      ],
      "metadata": {
        "id": "n17tSjeVxGWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name=''\n",
        "\n",
        "cfg=True\n",
        "change_query=True\n",
        "COVE=True\n",
        "MT=True\n",
        "\n",
        "#process_questions(['Identify one team with highest open issues over 8 months'])\n",
        "\n",
        "if(cfg):\n",
        "  name+='_CFG'\n",
        "if(change_query):\n",
        "  name+='_Query'\n",
        "if(COVE):\n",
        "  name+='_COVE'\n",
        "if(MT):\n",
        "  name+='_MT'\n",
        "\n",
        "if(cfg and change_query and COVE and MT):\n",
        "  name='_CIMBUR'\n",
        "\n",
        "if not (cfg or change_query or COVE or MT):\n",
        "    name = '_LLM'\n",
        "\n",
        "print(name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzSWA7jfnvCj",
        "outputId": "94f34b23-5234-4c0c-9a0e-27d81571f4f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_CIMBUR\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Benchmark/E-T1S.csv')\n",
        "\n",
        "single_q = df.loc[df['T1S-SQ'].notnull(), 'T1S-SQ']\n",
        "single_a=process_questions(single_q)\n",
        "\n",
        "df[f'T1S-SA{name}'] = ''\n",
        "df.loc[:len(single_a)-1, f'T1S-SA{name}'] = single_a\n",
        "\n",
        "df.to_csv('Benchmark/E-T1S.csv', index=False)\n",
        "\n",
        "# Get actual and expected answers\n",
        "actual_answers = single_a\n",
        "expected_answers = df.loc[df['T1S-SA'].notnull(), 'T1S-SA']\n",
        "\n",
        "# Call the function\n",
        "results, correct, correctness = evaluate_summary(actual_answers, expected_answers)\n",
        "print(\"Correctness:\", correctness)\n",
        "\n",
        "df[f'T1S-S{name}_R'] = ''\n",
        "df[f'T1S-S{name}_R'][:len(results)] = results\n",
        "\n",
        "df.to_csv('Benchmark/E-T1S.csv', index=False)\n",
        "\n",
        "result_df[f'T1S-S{name}']=[correctness]\n",
        "result_df.to_csv('Result/Correctness.csv', index=False)\n",
        "\n",
        "count_df[f'T1S-S{name}']=[correct]\n",
        "count_df.to_csv('Correct_Count.csv', index=False)"
      ],
      "metadata": {
        "id": "nxOdZJCGxFuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###fact"
      ],
      "metadata": {
        "id": "4Kp9TMwB5qNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Benchmark/E-T1S-F.csv')\n",
        "\n",
        "single_q = df.loc[df['T1S-FQ'].notnull(), 'T1S-FQ']\n",
        "single_a=process_questions(single_q)\n",
        "\n",
        "df[f'T1S-FA{name}'] = ''\n",
        "df.loc[:len(single_a)-1, f'T1S-FA{name}'] = single_a\n",
        "\n",
        "df.to_csv('Benchmark/E-T1S-F.csv', index=False)\n",
        "\n",
        "# Get actual and expected answers\n",
        "actual_answers = single_a\n",
        "expected_answers = df.loc[df['T1S-FA'].notnull(), 'T1S-FA']\n",
        "\n",
        "# Call the function\n",
        "results, correct, correctness = evaluate_summary(actual_answers, expected_answers)\n",
        "print(\"Correctness:\", correctness)\n",
        "\n"
      ],
      "metadata": {
        "id": "Dn_C4kOb5zI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[f'T1S-FA{name}_R'] = ''\n",
        "df[f'T1S-FA{name}_R'][:len(results)] = results\n",
        "\n",
        "df.to_csv('Benchmark/E-T1S-F.csv', index=False)\n",
        "\n",
        "\n",
        "result_df[f'T1S-F{name}']=[correctness]\n",
        "result_df.to_csv('Result/Correctness.csv', index=False)\n",
        "\n",
        "count_df[f'T1S-F{name}']=[correct]\n",
        "count_df.to_csv('Correct_Count.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aG51i3fipZS5",
        "outputId": "3be3f57b-6a5c-4ca4-f209-4918b2aae4dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-331-8182bde49aa6>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[f'T1S-FA{name}_R'][:len(results)] = results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluation"
      ],
      "metadata": {
        "id": "jYZK8rnZZ4_W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###y/n"
      ],
      "metadata": {
        "id": "n67SSfW6xUR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "classifier = pipeline(\"zero-shot-classification\")"
      ],
      "metadata": {
        "id": "GHBLggvjjtGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def classify_and_evaluate(actual_answers, expected_answers):\n",
        "    # Initialize the zero-shot classification pipeline\n",
        "\n",
        "\n",
        "    # Define classes\n",
        "    classes = [\"Yes\", \"No\"]\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    results = []\n",
        "\n",
        "    for actual, expected in zip(actual_answers, expected_answers):\n",
        "        print(actual)\n",
        "        print(expected)\n",
        "        total += 1\n",
        "        result = classifier(actual, candidate_labels=classes, hypothesis_template=\"This statement implies: {}.\")\n",
        "        if result['labels'][0].lower() == expected.lower():\n",
        "            correct += 1\n",
        "            results.append(1)\n",
        "            print(1)\n",
        "        else:\n",
        "            results.append(0)\n",
        "            print(0)\n",
        "\n",
        "    print('Correct '+ str(correct))\n",
        "    correctness = correct / total\n",
        "    return results, correct, correctness\n",
        "\n"
      ],
      "metadata": {
        "id": "H1R7EJt5bcIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###fact"
      ],
      "metadata": {
        "id": "znbdOVdcxXOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence-transformers"
      ],
      "metadata": {
        "id": "H7WbwsFJhTdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def extract_numbers(text):\n",
        "    return re.findall(r'\\b\\d+\\b', text)\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^\\w\\s:]', '', text)\n",
        "    text = text.strip().replace('\\n', '').replace('\"', '').replace(\"'\", '')\n",
        "    return text\n",
        "\n",
        "def containsOnlyNumbers(text):\n",
        "\n",
        "\n",
        "  text = re.sub(r'[^\\w\\s]', '', text)\n",
        "  text = text.replace('\\n', '').replace('and', '')\n",
        "\n",
        "  all_numbers = ' '.join(extract_numbers(text))\n",
        "\n",
        "  print('=------erar-----')\n",
        "  print(text)\n",
        "  print(all_numbers)\n",
        "\n",
        "  if(text==all_numbers):\n",
        "        print(text)\n",
        "        return True\n",
        "\n",
        "  return False\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_fact(actual_answers, expected_answers):\n",
        "\n",
        "    results = []\n",
        "    correct=0\n",
        "    # Evaluate correctness through semantic similarity\n",
        "    for actual, expected in  zip(actual_answers, expected_answers):\n",
        "\n",
        "        if(actual==None and expected==None):\n",
        "          return\n",
        "        if(actual==None):\n",
        "          actual='None'\n",
        "\n",
        "        expected=expected.strip()\n",
        "        actual=actual.strip()\n",
        "        similarity=0\n",
        "\n",
        "        if(expected.isdigit() or len(expected.split())==1):\n",
        "          print('first case')\n",
        "\n",
        "          actual_s=set(clean_text(actual).split())\n",
        "          expected_s=set(clean_text(expected).split())\n",
        "\n",
        "          actual_s = set(word.lower() for word in actual_s)\n",
        "          expected_s = set(word.lower() for word in expected_s)\n",
        "\n",
        "          intersection = actual_s.intersection(expected_s)\n",
        "\n",
        "          print(actual)\n",
        "          print(expected)\n",
        "\n",
        "          if intersection ==  set(expected.split()) or intersection:\n",
        "            print('intersection')\n",
        "            print(intersection)\n",
        "            similarity=1\n",
        "\n",
        "\n",
        "        elif containsOnlyNumbers(expected):\n",
        "\n",
        "              print('second case')\n",
        "\n",
        "\n",
        "              expected_num=sorted(extract_numbers(expected))\n",
        "              expected_s = ' '.join(expected_num)\n",
        "              min_limit=min(expected_num)\n",
        "              max_limit=max(expected_num)\n",
        "\n",
        "              actual_num=sorted(extract_numbers(actual))\n",
        "              if(len(actual_num)!=0):\n",
        "\n",
        "                actual_num = [num for num in actual_num if num > min_limit or num<=max_limit]\n",
        "                actual_s = ' '.join(actual_num)\n",
        "                print(actual_s)\n",
        "                actual_embedding = model.encode(actual_s, convert_to_tensor=True)\n",
        "                expected_embedding = model.encode(expected_s, convert_to_tensor=True)\n",
        "                similarity = util.pytorch_cos_sim(actual_embedding, expected_embedding).item()\n",
        "\n",
        "\n",
        "\n",
        "              print(expected_s)\n",
        "              print(similarity)\n",
        "\n",
        "        else:\n",
        "              print('third case')\n",
        "              actual_embedding = model.encode(actual, convert_to_tensor=True)\n",
        "              expected_embedding = model.encode(expected, convert_to_tensor=True)\n",
        "              similarity = util.pytorch_cos_sim(actual_embedding, expected_embedding).item()\n",
        "\n",
        "              print(actual)\n",
        "              print(expected)\n",
        "\n",
        "\n",
        "\n",
        "        threshold = 0.7\n",
        "        print(similarity)\n",
        "        is_correct = 1 if similarity >= threshold else 0\n",
        "        print(is_correct)\n",
        "        if(is_correct==1):\n",
        "          correct=correct+1\n",
        "        results.append(is_correct)\n",
        "\n",
        "\n",
        "    print('Correct '+ str(correct))\n",
        "    correctness=correct/len(results)\n",
        "    return results, correct, correctness"
      ],
      "metadata": {
        "id": "pFFmplKXDkFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###sum"
      ],
      "metadata": {
        "id": "SdRrOTvfxZYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "1D4zYBlozqFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def evaluate_summary(actual_answers, expected_answers):\n",
        "\n",
        "    results = []\n",
        "    correct=0\n",
        "\n",
        "\n",
        "    # Evaluate correctness through semantic similarity\n",
        "    for actual, expected in  zip(actual_answers, expected_answers):\n",
        "        if(actual==None):\n",
        "          actual='None'\n",
        "        actual_embedding = model.encode(actual, convert_to_tensor=True)\n",
        "        print(actual)\n",
        "        expected_embedding = model.encode(expected, convert_to_tensor=True)\n",
        "        print(expected)\n",
        "        similarity = util.pytorch_cos_sim(actual_embedding, expected_embedding).item()\n",
        "\n",
        "        # Assuming a threshold for \"correctness\" - this threshold is arbitrary and can be adjusted\n",
        "        threshold = 0.7\n",
        "        print(similarity)\n",
        "        print(round(similarity))\n",
        "        is_correct = 1 if similarity >= threshold else 0\n",
        "        print(is_correct)\n",
        "        if(is_correct==1):\n",
        "          correct=correct+1\n",
        "\n",
        "        results.append(is_correct)\n",
        "    print('Correct '+ str(correct))\n",
        "    correctness=correct/len(results)\n",
        "    return results, correct, correctness"
      ],
      "metadata": {
        "id": "n22dLsBbxyFU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}